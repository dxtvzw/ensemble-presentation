@misc{allenzhu2023understanding,
      title={Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning}, 
      author={Zeyuan Allen-Zhu and Yuanzhi Li},
      year={2023},
      eprint={2012.09816},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
},
@article{breiman96,
  added-at = {2011-05-12T20:52:32.000+0200},
  author = {Breiman, Leo},
  biburl = {https://www.bibsonomy.org/bibtex/2d7f2fcb651038d60087aa9d4a046eef8/smolav},
  interhash = {96b419212a439d785711b0d79e00332d},
  intrahash = {d7f2fcb651038d60087aa9d4a046eef8},
  journal = {Machine Learning},
  keywords = {bagging ensemble in-thesis machine-learning projn2011},
  number = 2,
  pages = {123-140},
  timestamp = {2011-07-28T23:34:55.000+0200},
  title = {Bagging Predictors},
  volume = 24,
  year = 1996
},
@article{FREUND1997119,
title = {A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting},
journal = {Journal of Computer and System Sciences},
volume = {55},
number = {1},
pages = {119-139},
year = {1997},
issn = {0022-0000},
doi = {https://doi.org/10.1006/jcss.1997.1504},
url = {https://www.sciencedirect.com/science/article/pii/S002200009791504X},
author = {Yoav Freund and Robert E Schapire},
abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.}
},
@article{random_forests,
author = {Breiman, L},
year = {2001},
month = {10},
pages = {5-32},
title = {Random Forests},
volume = {45},
journal = {Machine Learning},
doi = {10.1023/A:1010950718922}
},
@article{gbm,
author = {Jerome H. Friedman},
title = {{Greedy function approximation: A gradient boosting machine.}},
volume = {29},
journal = {The Annals of Statistics},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {1189 -- 1232},
keywords = {boosting, decision trees, Function estimation, robust nonparametric regression},
year = {2001},
doi = {10.1214/aos/1013203451},
URL = {https://doi.org/10.1214/aos/1013203451}
},
@article{WOLPERT1992241,
title = {Stacked generalization},
journal = {Neural Networks},
volume = {5},
number = {2},
pages = {241-259},
year = {1992},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(05)80023-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608005800231},
author = {David H. Wolpert},
keywords = {Generalization and induction, Combining generalizers, Learning set preprocessing, cross-validation, Error estimation and correction},
abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.}
},
@misc{sanh2020distilbert,
      title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
      author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
      year={2020},
      eprint={1910.01108},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
},
@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
},
@article{ntk1,
author = {Wang, Dianhui and Alhamdoosh, Monther},
year = {2013},
month = {02},
pages = {98–110},
title = {Evolutionary Randomized Neural Network Ensembles with Size Control},
volume = {102},
journal = {Neurocomputing},
doi = {10.1016/j.neucom.2011.12.046}
}